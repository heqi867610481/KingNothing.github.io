<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title></title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title">
<meta property="og:url" content="https://heqi867610481.github.io/KingNothing.github.io/index.html">
<meta property="og:site_name">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Meiyi Jiang">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="https://heqi867610481.github.io/KingNothing.github.io/atom.xml" title="" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
<link rel="stylesheet" href="https://heqi867610481.github.io/KingNothing.github.io/css/style.css">

<meta name="generator" content="Hexo 4.2.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="https://heqi867610481.github.io/KingNothing.github.io/" id="logo"></a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="https://heqi867610481.github.io/KingNothing.github.io/">Home</a>
        
          <a class="main-nav-link" href="https://heqi867610481.github.io/KingNothing.github.io/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="https://heqi867610481.github.io/KingNothing.github.io/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://heqi867610481.github.io/KingNothing.github.io"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-paper10" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="https://heqi867610481.github.io/KingNothing.github.io/2020/06/28/paper10/" class="article-date">
  <time datetime="2020-06-28T12:11:22.000Z" itemprop="datePublished">2020-06-28</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="https://heqi867610481.github.io/KingNothing.github.io/2020/06/28/paper10/">论文笔记：10   Latent Backdoor Attacks on Deep Neural Networks</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="1-论文总体思路"><a href="#1-论文总体思路" class="headerlink" title="1 论文总体思路"></a>1 论文总体思路</h1><p>对于神经网络的后门攻击也是一个攻击的类别，指的是在正常的模型中隐藏着误分类的规则，仅仅在特定的输入时被触发。在本文中提出了latent backdoors，一种基于迁移学习的后门攻击。后门嵌入在Teacher模型中，当通过迁移学习迁移到Student模型中时被激活。目的是使得任意有着设计好的patten的图片都被分类为target类别（如果student模型中恰好就包含了target类别的话）。实验部分针对信号灯识别，虹膜识别和公众人物的人脸识别做了实验验证。此外还提出了4中潜在的防御机制，发现仅一种是有效的，但分类的正确率会受影响。<br>一般而言，迁移学习增加了后门攻击的难度。因为往往针对的target类别在公开的teacher模型中还不存在，以及嵌入的触发机制在迁移学习过程中往往会遭到破坏。但本文设计的后门攻击是被嵌入在teacher模型中，针对的target标签也是一个在teacher模型中不存在的标签，然后经过迁移学习后门也不损坏，最终在student模型中被触发。<br>本文的方法在以下几方面优于前人：1）后门可以在teacher模型中一直潜伏，经过迁移学习也不受破坏。2）因为不针对在teacher模型中存在的标签，所以不会被普通的输入测试所发现。3）迁移学习可以放大后门的影响范围，因为每个student模型都暗含后门。4）攻击更先发制人，target标签可以由攻击者指定，并期待着在student模型上被激活，而传统的仅仅能从已经存在的标签中选择target标签。<br>攻击的设计包含源自两个事情：1）传统的后门直接将触发器和输出的分类标签相联系，而本文将触发和将导致某分类标签的中间层特征表示相联系。这样即使迁移学习后输出标签被改变也能被触发。2）嵌入触发来在深度神经网络的中间层产生一个匹配的特征表达，只要迁移学习没有改变到这一层就没有影响。</p>
<h1 id="2-相关背景"><a href="#2-相关背景" class="headerlink" title="2 相关背景"></a>2 相关背景</h1><h2 id="2-1-DNN后门攻击"><a href="#2-1-DNN后门攻击" class="headerlink" title="2.1 DNN后门攻击"></a>2.1 DNN后门攻击</h2><p>Gu et al.通过毒化训练集来进行后门注入，通过给部分训练集加trigger pattern并将他们的标签设为目标标签，接下来用这些数据进行训练。<br>Liu et al.提出的方案需要更少的训练数据。他们试图构建对某些神经元有重要反应的特殊触发器。</p>
<h2 id="2-2-现存的防御"><a href="#2-2-现存的防御" class="headerlink" title="2.2 现存的防御"></a>2.2 现存的防御</h2><p>Neuron Cleanse通过检查模型的输出标签并且逆向任何潜在的隐藏的触发器。直觉解释是对于被后门针对的那个标签而言，要使得这个标签下所有输入误分类的扰动应当比正常标签的小。<br>Activation Clustering。直觉解释是由毒化输入产生的被激活的神经元的pattern和正常的不一样 。<br>Fine-Pruning。是移除后门的方法。通过修建对分类最没用的冗余神经元，再用正常数据微调模型来维持模型的表现。</p>
<h1 id="3-攻击细节"><a href="#3-攻击细节" class="headerlink" title="3 攻击细节"></a>3 攻击细节</h1><p>攻击者不要求有关于target标签的图片的具体知识，从 公共资源搜集到的图片就可以。teacher任务和student任务也不需要严格匹配。当两个任务不同时，攻击者仅需要再收集一些和student任务相似的任务的样本就行。攻击的操作在teacher模型上完成，不改变student模型的训练数据和过程。然后迁移学习中，有后门的teacher模型应该和正常的模型在正常数据集上表现一样。<br>攻击实施的主要挑战有以下两点：1）因为teacher模型没有目标标签，攻击者不能用现存的用目标标签的方法注入后门。2）因为迁移学习要改变部分的teacher模型，可能会打乱触发器和目标类别的联系。</p>
<h2 id="3-1-攻击流程"><a href="#3-1-攻击流程" class="headerlink" title="3.1 攻击流程"></a>3.1 攻击流程</h2><p>步骤1：修改teacher模型使得输出的标签中含有目标标签yt。<br>攻击者用两个和目标任务相关的数据集来重新训练原来的teacher模型。一个数据是属于yt标签的干净的数据，记作目标数据Xyt，另一个是和目标任务相近的干净的数据，记作非目标数据集X\yt。然后用新的最后一层分类层替换掉原来的分类层。<br>步骤2：生成后门触发器$\Delta$。<br>这里给出来一系列备选的Kt值（指的是将要嵌入后门的哪个中间层）。然后对于攻击者指定的图片上某一区域和形状，计算触发器$\Delta$的纹理颜色等，使得对yt的效果最大化，这一步优化产生的触发器能够使任何输入的中间层Kt上的特征表示和yt标签下的干净样本的Kt层特征表示相似。<br>步骤3：注入后门触发器。<br>这一步骤中，攻击者用优化过程来更新模型权重，使得对抗样本的中间层表示和目标类别的中间层表示相匹配。过程中使用的数据集是干净的Xyt和加过pattern的X\yt。值得注意的是，选取能实验成功的尽可能小的Kt值，即层数越浅越好，从而使得迁移学习的过程尽量成功。<br>步骤4：从Teacher模型中除去yt的痕迹。<br>把最后的输出层再替换成原来的输出层，因为模型的权重已经变化了，攻击者可以在训练集上微调模型的最后一层权重。这样的结果就是模型还有着正常的准确率，但是已经被植入后门。</p>
<h2 id="3-2-攻击中的优化公式"><a href="#3-2-攻击中的优化公式" class="headerlink" title="3.2 攻击中的优化公式"></a>3.2 攻击中的优化公式</h2><p>基于目标的触发器生成步骤（对应于步骤2）：<br>如前所述，这一步骤的优化目标是寻找一个触发器pattern$\Delta$，使得毒化的非目标样本和任意干净的目标样本在Kt中间层的特征表示向量间差别最小。公式表示如下图所示。<br><img src="https://heqi867610481.github.io/KingNothing.github.io/KingNothing.github.io/2020/06/28/paper10/images/16.jpg" alt="公式"><br>其中计算差别使用的是平均平方差。需要注意的是，在这一步骤中不涉及到对teacher模型的改动。<br>后门注入步骤（对应于步骤3）：<br>攻击者更新teacher模型权重来进一步最小化中间层之间的差别。公式表示如下图：<br><img src="https://heqi867610481.github.io/KingNothing.github.io/KingNothing.github.io/2020/06/28/paper10/images/17.jpg" alt="公式"><br>其中损失函数的第一项是模型训练的标准损失函数，第二项最小化了毒化样本和目标样本中间层特征的差异。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://heqi867610481.github.io/KingNothing.github.io/2020/06/28/paper10/" data-id="ckczzvb1o00005kcpgnntf98l" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="https://heqi867610481.github.io/KingNothing.github.io/tags/%E5%90%8E%E9%97%A8%E6%94%BB%E5%87%BB/" rel="tag">后门攻击</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="https://heqi867610481.github.io/KingNothing.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" rel="tag">神经网络</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="https://heqi867610481.github.io/KingNothing.github.io/tags/%E8%AE%BA%E6%96%87/" rel="tag">论文</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-paper9" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="https://heqi867610481.github.io/KingNothing.github.io/2020/06/25/paper9/" class="article-date">
  <time datetime="2020-06-25T13:29:07.000Z" itemprop="datePublished">2020-06-25</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="https://heqi867610481.github.io/KingNothing.github.io/2020/06/25/paper9/">论文笔记：9   Neural Network Inversion in Adversarial Setting via Background Knowledge Alignment</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="1-论文总体思路"><a href="#1-论文总体思路" class="headerlink" title="1 论文总体思路"></a>1 论文总体思路</h1><p>本文实施了模型反演攻击，即攻击者意图从模型的预测值来推断模型的训练数据和测试数据。实施攻击采用的方法是再训练一个神经网络作为目标模型的反演模型。训练反演模型本文提出了两个技巧：1）利用攻击者的背景知识构建一个辅助数据集，而不需要得到受害者模型的训练集。2）设计了一种基于截断的技术，从而可以利用受害者模型的经过截断的部分预测值来实施模型反演。本文证实即使对于黑盒受害者模型的数据集仅有部分知识以及仅知道经过截断的预测值，反演模型仍然可以有着较好的表现，超越前人的方法。<br>模型反演是意图从模型的预测值当中获取训练数据的相关信息的技术，大致可分为两类：1）基于优化的方法，在数据空间中利用基于梯度的优化方法来反演模型。比如文中引用的文献21的MIA方法，就是把反演任务作为一个优化问题去找给定类别的最优数据。此方法对于简单的网络有效，但是被证明对于复杂的网络效果不佳，这可能是因为优化目标并没有真正抓住数据空间的语义信息。2）基于训练的方法，通过训练另一个表现为受害模型的反演的模型来实现。目标在于用包括神经网络每层的激活在内的计算机视觉特征来重构图片。<br>对于模型反演攻击的目标又可细分为两类：1）数据重构。给出分类器的预测向量，需要重构出这个未知的数据。比如对于人脸识别系统而言，就给出一个人的身份要求重构出此人的面部图片。2）训练类别推断。要求对于分类器的每个类别恢复一个语义上有意义可识别的数据。<br>本文设置的攻击者背景大多数情况是黑盒的，采用的是基于训练的攻击。为了解决不能得到训练数据的问题，方法是基于背景知识从一个更通用的数据分布中形成辅助数据集。这些辅助样本也保留有更通用普遍的特征。文中还有一个实验表明，即使在一些场合下攻击者可以获得受害模型的训练集，用通用样本对训练集做数据增强也能很好的提升准确率。<br>前面提到本文采用基于截断的方法来训练反演模型，核心思想是在训练过程中用截断后的预测输入。截断还能帮助减小过拟合。比如对于第二个问题训练类别推断，在训练过程中，直接将预测截断成独热向量来输入。这样训练出的反演模型可以由独热向量生成图片。<br>设计攻击者模型时考虑以下场景：1）攻击者是个普通用户，模型对他而言是个黑盒。2）攻击者即是开发者，那么反演模型就可以和分类器在同一个训练集上一起联合训练，这样产生的反演模型更精确。这种情况下，目标是迫使分类器的预测值也保留帮助反演模型重构数据的重要信息。使用反演模型的重构损失来限制分类器的训练。</p>
<h1 id="2-背景知识"><a href="#2-背景知识" class="headerlink" title="2 背景知识"></a>2 背景知识</h1><h2 id="2-1-模型反演"><a href="#2-1-模型反演" class="headerlink" title="2.1 模型反演"></a>2.1 模型反演</h2><p>通常计算机视觉领域的模型反演用图片的计算机视觉特征比如HOG和SIFT，或者用网络每一层的激活来重构图片。前面提到分成两种，基于优化的和基于训练的。基于优化的往往需要对白盒模型实施，而且对于大型神经网络倾向于生成不像自然图片的图片。此外因为要计算梯度，因此在测试集上也是很费时的。而基于训练的方法只在训练反演模型时耗时，而训练完在测试集上就很快。</p>
<h2 id="2-2-转置卷积（反卷积）"><a href="#2-2-转置卷积（反卷积）" class="headerlink" title="2.2 转置卷积（反卷积）"></a>2.2 转置卷积（反卷积）</h2><p>这个是构建反演模型的基础，本文并没有对其原理详细解释，只是应用了现成的模型。这里补充下转置卷积的原理。<br><a href="https://blog.csdn.net/lanadeus/article/details/82534425" target="_blank" rel="noopener">https://blog.csdn.net/lanadeus/article/details/82534425</a><br>首先转置卷积是一种上采样方法。上采样比如用来提高图片的分辨率。常见的方法有最近邻插值，双线性插值，双立方插值。这些方法不涉及到学习过程。如果想利用神经网络进行上采样，就需要使用转置卷积。<br>首先考虑正向的卷积操作。给定一个input，一个kernel，得出一个output矩阵。事实上，这一个过程可以表示为矩阵乘法，当然参与乘法的矩阵是经过数值重新排列以及0填充的。（详细过程见链接）。因此可以想象，两边同乘由kernel衍生矩阵的转置那样的维度的矩阵，就能实现卷积的逆过程。卷积是个多对一的过程，而转置卷积则是一对多。从信息论的角度看,卷积是不可逆的。所以这里说的并不是从output矩阵和kernel矩阵计算出原始的input矩阵。而是计算出一个保持了位置性关系的矩阵。也就是说用来进行转置卷积的权重矩阵不一定来自于原卷积矩阵。重点是权重矩阵的形状和转置后的卷积矩阵相同。需要注意的是，转置卷积会在生成的图像中造成棋盘效应。可以在使用转置卷积进行上采样操作之后再过一个普通的卷积来减轻此类问题。</p>
<h1 id="3-攻击者模型"><a href="#3-攻击者模型" class="headerlink" title="3 攻击者模型"></a>3 攻击者模型</h1><p>为攻击者设计了三个攻击场景。1）对黑盒分类器基于截断向量进行数据重构。2）对黑盒分类器进行功能推断。3）恶意开发者意图构造能帮助重构用户输入数据的分类器。</p>
<h2 id="3-1-场景一"><a href="#3-1-场景一" class="headerlink" title="3.1 场景一"></a>3.1 场景一</h2><p>这种情景下，攻击者不知道受害模型训练集的分布，架构以及参数，但是有一些最基础的背景知识。比如攻击者知道这大概是一个人脸识别的分类器，这样他可以从一个更通用的分布中获得样本。攻击者还知道分类器的输入格式和输出格式（输出向量的维度）。关于输出维度，对于被截断的向量也是可行的。因为可以用一系列输入数据送去查询然后发现一共都有哪些类。这个场景下给出一个被截断的预测向量f，攻击者要找到一个最合适的数据使得被分类器分类结果仍是f。</p>
<h2 id="3-2-场景二"><a href="#3-2-场景二" class="headerlink" title="3.2 场景二"></a>3.2 场景二</h2><p>与前者不同，给出一个类别y，攻击者意图找到该类的代表性数据，也就是找到一个数据使得被分类器分为类别y。</p>
<h2 id="3-3-场景三"><a href="#3-3-场景三" class="headerlink" title="3.3 场景三"></a>3.3 场景三</h2><p>这场景下，攻击者即开发者，知道模型的训练集，架构，参数。攻击者可以获得用户的被截断的预测向量，然后试图重构用户输入数据。攻击者的目标是还保证原模型的分类准确率的情况下，同时尽量提高数据重构的质量。</p>
<h1 id="4-实施方法"><a href="#4-实施方法" class="headerlink" title="4 实施方法"></a>4 实施方法</h1><h2 id="4-1-辅助数据集（没啥好说的，略）"><a href="#4-1-辅助数据集（没啥好说的，略）" class="headerlink" title="4.1 辅助数据集（没啥好说的，略）"></a>4.1 辅助数据集（没啥好说的，略）</h2><h2 id="4-2-反演模型的截断方法"><a href="#4-2-反演模型的截断方法" class="headerlink" title="4.2 反演模型的截断方法"></a>4.2 反演模型的截断方法</h2><p><img src="https://heqi867610481.github.io/KingNothing.github.io/KingNothing.github.io/2020/06/25/paper9/images/13.jpg" alt="反演模型训练过程"><br>对于辅助集的样本，截断他们的预测向量到同样的维度，然后作为输入来训练反演模型，这样可以迫使用截断的预测来重构数据。反演模型训练时候最小化的目标函数表示如下。<br><img src="https://heqi867610481.github.io/KingNothing.github.io/KingNothing.github.io/2020/06/25/paper9/images/14.jpg" alt="公式"><br>其中a是出自辅助集的样本，R是损失函数，本文中用的是L2。这个截断过程，可以看作是正向训练的特征选择过程，去除不重要的特征，所以也就能理解为什么这一步能减少反演模型的过拟合。对于前文提到的攻击场景二，训练类别推断，可以看作向量被截断成维度为1。</p>
<h2 id="4-3-联合训练分类器和反演模型"><a href="#4-3-联合训练分类器和反演模型" class="headerlink" title="4.3 联合训练分类器和反演模型"></a>4.3 联合训练分类器和反演模型</h2><p>这种情况下，除了训练分类器的损失函数，还有重构损失函数，描述如下：<br><img src="https://heqi867610481.github.io/KingNothing.github.io/KingNothing.github.io/2020/06/25/paper9/images/15.jpg" alt="公式"><br>其中，D是训练集。直觉上解释，有了这个重构损失能使得分类器的预测倾向于保留有重要的信息。实验过程中发现直接使用预测向量效果不够好，这是因为输出被softmax限制在了0~1，这实际上削弱了输出的激活，造成了信息损失。为了解决这个问题，将输出的预测向量重新缩放到一个更大的范围，其中的参数在训练反演模型过程中优化。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://heqi867610481.github.io/KingNothing.github.io/2020/06/25/paper9/" data-id="ckczzvb1x00015kcp8am255zj" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="https://heqi867610481.github.io/KingNothing.github.io/tags/%E8%AE%BA%E6%96%87/" rel="tag">论文</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-paper8" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="https://heqi867610481.github.io/KingNothing.github.io/2020/06/15/paper8/" class="article-date">
  <time datetime="2020-06-15T10:39:54.000Z" itemprop="datePublished">2020-06-15</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="https://heqi867610481.github.io/KingNothing.github.io/2020/06/15/paper8/">论文笔记：8   CloudLeak：Large-Scale Deep Learning Models Stealing Through Adversarial Examples</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>BGM：Tatoue-moi</p>
<h1 id="1-论文概述"><a href="#1-论文概述" class="headerlink" title="1 论文概述"></a>1 论文概述</h1><p>本文提出了一种针对DNN的有效的黑盒攻击方式。对于基于云的MLaaS可以以近乎完美的效果训练出替代模型。和现有的方法比较，查询次数明显少。攻击整合了多种主动学习、迁移学习、对抗攻击等。实验过程在Microsoft，Face++，IBM，Google和Clarifai等商业平台上进行了验证。<br>对于商用的模型，已经有一些攻击方式可以进行模型提取以及数据窥探。比如，F.Tramer et al.第一个提出了一种模型提取攻击，通过查询和获得预测结果，可以本地训练一个近乎等同的机器学习模型。R.Shokri et al.提出了一种成员推断攻击，来确定训练集中是否有特定的数据记录。别人进一步的研究成果表明，成员暴露广泛存在，不仅存在于过拟合的模型。针对此类攻击的防御机制也有被提出，但不能同时保证效果和效率。<br>尽管DNN查询和模型提取攻击有重大进展，但是实现现实生活中的攻击仍然是不现实的，这是因为1）当前的模型提取攻击主要针对小型ML模型，比如逻辑回归、支持向量机和小型神经网络。但对于更复杂的DNN模型，结果有效性不明。2）当前的模型提取攻击需要的查询次数正比于目标模型的参数个数，对于流行的DNN模型比如VGGNet，ResNet和Inception等有着上百万参数的模型而言就很不实际。所以现存的方法表现比较好的话，就要求大量的查询次数。<br>本文提出的方案的核心思想是向受害模型输入恶意样本，获得输出结果，用这输入-输出对来重新训练本地的替代模型（替代模型从 candidate Model Zoo中选择）。通过应用一种基于边的，对抗性的，主动学习的算法，来搜寻恶意样本，这样可以提高查询的效率。这种方法搜寻到的图片几乎分布在受害模型的分类边界线上，这样攻击者也可以降低构造合成数据集时候的label工作。<br>本文的贡献总结如下：1）针对本地替代模型，提出一种名为Feature Fool的对抗攻击方法，采用内部表示来生成恶意样本的子集（比如合成数据集）。这些恶意样本被用来查询受害者模型的决策层边界线间的距离。2）设计了一种针对大规模DNN的黑盒模型窃取攻击方式。攻击通过在训练好的模型的基础上应用对抗性主动学习和迁移学习，有效的加速了模型窃取进程。3）在一组商业平台上进行了评估，效果和效率可以同时保证。</p>
<h1 id="2-背景知识"><a href="#2-背景知识" class="headerlink" title="2 背景知识"></a>2 背景知识</h1><h2 id="2-1-迁移学习"><a href="#2-1-迁移学习" class="headerlink" title="2.1 迁移学习"></a>2.1 迁移学习</h2><p>关于迁移学习的原理此处就不赘述了。但是论文提到了Sun et al.做的工作和本文有点像。Sun设计了DeepID来学习一系列的高级特征表示，并做了基于DeepID的联合贝叶斯模型从源域到目标域的转换。与Sun的工作不同的是，本文在一个训练子集上对VGG19模型做了微调，然后使用DeepID来进一步提取高级特征表示。这个模型在文中被称为VGG_DeepID。<br>关于DeepID的内容，这里引用一下链接里的原文。<br><a href="https://zhuanlan.zhihu.com/p/82448662" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/82448662</a><br>“此文发表于2014年CVPR上，以人脸识别的子领域人脸验证(Face Verification)为目标，文章并没有用直接用二类分类CNN实现人脸验证，而是通过学习一个多类（10000类，每个类大约有20个实例）人脸识别任务来学习特征，并把学到的特征使用到face verification和其他unseen新的identification。它使用人脸上不同的patch训练多个单独的ConvNet，每个ConvNet的最后一个隐层为提取到的特征，称之为DeepID(Deep hidden IDentitiy feature)。最后将这些patch提取到的DeepID concate起来作为整个Face的feature送入Joint Bayesian分类器（当然也可以是其他的分类器）做二分类就可以做人脸验证了。文章主要的贡献在于用很多不同的Face patch分别训练以一个很难的分类任务（~10000个不同身份的人）的ConvNet，得到一些分辨力很强的over-complete representations，最后在不需要严格对齐的前提下于LFW上取得了97.45%的人脸对比精度。”<br>本文应用了迁移学习。在源任务方面，用了AlexNet，VGG19，VGGFace和ResNet50作为基础架构。其中，对于VGG19，移除了其中的全连接层FC6，添加了一个DeepID层，DeepID层结合了之前最大池化层和卷积层的特征。这个DeepID层在VGG19模型的最后部分，接着是两个全连接层FC7和FC8。前面卷积层的权重和偏差是在ImageNet上训练的，被源任务和目标任务所共享，而DeepID层和两个全连接层会在合成数据集上微调。</p>
<h2 id="2-2-对抗样本"><a href="#2-2-对抗样本" class="headerlink" title="2.2 对抗样本"></a>2.2 对抗样本</h2><p>关于对抗样本，一些认为替代模型有着和受害模型相似的边界，并且为替代模型生成的对抗样本也能够很好的迁移到其他label。也有一些研究通过操纵DNN的内部特征来获得更好的攻击表现。本文也采取了使用DNN的内部特征的相似方法，来生成对抗样本。和现有的特征级别的对抗攻击不同，本文主要集中于以下两方面。1）生成的恶意特征的使用。现有的特征级攻击方法仅使用引导图像的特征表示来生成对抗性示例，而不生成使用显著性映射计算的恶意特征表示。2）求解模型参数以最小化目标类的置信度得分。</p>
<h2 id="2-3-主动学习"><a href="#2-3-主动学习" class="headerlink" title="2.3 主动学习"></a>2.3 主动学习</h2><p>主动学习是迭代性的选择信息样本来教给人们label，同时最大化重新训练DNN的表现。但是之前的不确定抽样方法有着一个问题，那就是被选择的几乎在分类的边界线上的样本过于相似，从而导致分类结果很差，但是人们又把这些样本看作理想的训练集。在本文中，通过一系列对抗样本生成算法，来提高分布在边界线上的有效样本的多样性，从而解决这个问题。<br><a href="https://baijiahao.baidu.com/s?id=1641909326857282702&wfr=spider&for=pc" target="_blank" rel="noopener">https://baijiahao.baidu.com/s?id=1641909326857282702&amp;wfr=spider&amp;for=pc</a></p>
<h1 id="3-攻击算法"><a href="#3-攻击算法" class="headerlink" title="3 攻击算法"></a>3 攻击算法</h1><h2 id="3-1-对抗性主动学习过程"><a href="#3-1-对抗性主动学习过程" class="headerlink" title="3.1 对抗性主动学习过程"></a>3.1 对抗性主动学习过程</h2><p>提出了一种基于边界的对抗性主动学习算法。给定一个无标签数据集（主动学习池），从中筛选出有效样本构成的子集，所谓有效，指的是对于决定受害分类器的分类边界有效或信息量大。由于多分类器可以被看作是一系列的二分类器，可以先用二分类来理解算法。攻击者生成的合成数据集的样本主要包含两类：有着最低置信的合法样本，和有着最低置信的攻击样本。与高置信的样本相比较，这些样本更容易提供关于超平面的信息。对于多分类问题，由于分界的超平面几何复杂性很高，提出FeatureFool方法来寻找使目标模型有着低置信的样本。</p>
<h2 id="3-2-对抗样本生成过程"><a href="#3-2-对抗样本生成过程" class="headerlink" title="3.2 对抗样本生成过程"></a>3.2 对抗样本生成过程</h2><p>首先讲一下本文提出的FF方法。<br>FF：FeatureFool方法基于的是L-BFGS方法。L-BFGS是在对抗样本的分类结果为目标类l的限制下，最小化扰动后样本和原样本间的损失函数。FeatureFool做了改进，利用了图片的内部特征。于是目标函数除了最小化原来那一项，还加了一个三联损失。三联损失是对抗样本分别和目标样本以及原样本的中间层特征距离差，最小化这一项意味着要求对抗样本和目标尽量相似，和源样本尽量远离。<br>其他用来做对比实验的方法如下：<br>RS：Random Sample，就是随机从原来的域中抽样，向受害模型查询，不生成对抗样本。<br>PGD：具体内容参见paper1，不详细说。<br>CW：关于CW的详细内容，可以翻看一下paper1里面。需要注意的是CW通过一个参数k来调整希望对抗样本的分类错的置信水平。所以就可以调整这个参数，来达到使对抗样本尽量落于边界的目的。<br>FA：Feature Adversary，是由Sabour et al. 提出的攻击方式，原理是最小化“图片对”的在受害模型中的内部特征向量的的Lp距离。</p>
<h2 id="3-3-DNN训练过程"><a href="#3-3-DNN训练过程" class="headerlink" title="3.3 DNN训练过程"></a>3.3 DNN训练过程</h2><p>对于以上5中数据集生成方法而言，RS是随机选择加入到训练集，并没有生成对抗样本，以下叙述不包含这种情况。对于其他4种方法来说。数据集的生成方法描述如下：<br>首先随机从目标域中选择一个小子集，作为最初始的数据集。用对抗样本生成算法对最初的替代模型作出攻击，由此得到一些恶意样本。然后把恶意样本送到受害模型中去，得到他的输出。将其加入到数据集中，完成一次对迭代生成合成数据集的过程。然后用合成的数据集微调本地的替代模型（迁移学习）。在这个过程中，替代模型和受害模型的分类边界逐渐相似。<br>这里再给出一个直观上的解释：如果替代模型和受害模型是二分类的线性分类器，在p维空间，那么，那么边界就是p维空间的超平面。那么如果我们数据集中有超过p个点位于边界上，那么就可以完完整整的恢复边界。另一方面，如果数据集里的点都是随即选的，那就需要更多的点来恢复边界。</p>
<h1 id="4-实验设置"><a href="#4-实验设置" class="headerlink" title="4 实验设置"></a>4 实验设置</h1><p>实验部分，作者训练了Microsoft Cloud Vision Service，IBM Watson Visual Recognition，Google AutoML Vision，他们分别针对的是交通标志识别，花朵识别，人脸识别。这模仿了用户上传私人数据集来训练云模型的过程。此外还有Face++ Emotion Recognition API和Clarifai Safe for Work (NSFW) API。这两个是直接当作黑盒攻击的，没有自己用数据集训练的过程。实验从两方面进行，一个是模型窃取攻击对不同数据集和迁移架构的攻击效果；另一个是展示了本文提出的攻击手法和已有的F.Tramer、Correia-Silva、Papernot等攻击的比较。<br>本地预训练模型有AlexNet、VGG19、VGGFace、ResNet50。预训练模型训练过程中首先从较大的学习率开始，然后逐渐减小。使用随机梯度下降法来最小化交叉熵损失，还用了剪裁、旋转和缩放等数据增强方法。<br>以下是一些比较重要的实验结论：1）对抗样本的扰动提高了合成数据集的多样性，使得迁移效果更好。但是这个优点随着查询次数的增多就不那么突出了。2）与RS、PGD、FA相比较，基于CW、FF训练的替代模型效果更好，这是因为这两种方法 都通过参数可以控制误分类的置信度。3）通过分析使用不同预训练模型作为替代模型的表现，可以观察到替代模型的表现可以被模型复杂性和任务相关度所影响，更复杂，数据集更相关的往往效果更好。<br>和已知攻击的对比，本文提出的方法是最好的，可以兼顾高准确率和更少的查询次数。Tramer的攻击效果不好可能是使用了线性搜寻来找样本，然而这导致找的样本有些过于相似。</p>
<h1 id="5-其他"><a href="#5-其他" class="headerlink" title="5 其他"></a>5 其他</h1><h2 id="5-1-防御"><a href="#5-1-防御" class="headerlink" title="5.1 防御"></a>5.1 防御</h2><p>首先应用了PRADA防御方法。这种防御的原理是分析对受害模型的查询的分布，根据它们之间的联系来检测模型提取攻击。但这种防御对于精心构造并没有使分类置信度大幅下降的样本的本文攻击而言不太有效，从而使得防御的误报率很高。<br>论文提出的潜在的防御机制不在于分析连续查询的分布，而在于检测对抗样本。基于对抗样本和正常样本的特征来进行检测。使用深度学习框架PyTorch训练了一个特征分布引导网络，起名叫DefenseNet。预训练好的DefenseNet被用来提取出每个隐藏层的输出作为输入样本的特征。利用分类混合模型作为先验概率来刻画这些查询样本的分布。攻击者生成的对抗性实例具有不同于良性样本分布的特征分布。作者还将支持向量机分类集成到DefenseNet中，以区分良性样本和对抗性样本，而不是可能改变决策边界的先前工作。</p>
<h2 id="5-2-局限与未来改进方向"><a href="#5-2-局限与未来改进方向" class="headerlink" title="5.2 局限与未来改进方向"></a>5.2 局限与未来改进方向</h2><p>1）首先一个主要的局限性是为了最大化远离觉决策边界线的样本的不确定性，攻击者会添加更多的扰动。但这样的话，有着很大扰动的对抗样本会慢慢的污染合成数据集，从而持续降低替代模型的准确性。未来可以设计更复杂的算法来在二者之间做个平衡。<br>2）未来可以把该方法拓展到多标签情境下。也就是说，为了对受害者模型发起模型窃取攻击，攻击者需要首先制作带有多目标标签的对抗样本，然后生成合成数据集来训练替代模型。虽然所提出的攻击框架没有在多标签分类模型上进行评估，但是本文所介绍的对抗式查询方法可以帮助对手获得关于受害者模型的更重要的信息，如决策边界、标签类型等。<br>3）未来还可以拓展到其他领域，比如语音和文本。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://heqi867610481.github.io/KingNothing.github.io/2020/06/15/paper8/" data-id="ckbgddaxe0000focp083cd4q7" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="https://heqi867610481.github.io/KingNothing.github.io/tags/%E5%AF%B9%E6%8A%97%E6%94%BB%E5%87%BB/" rel="tag">对抗攻击</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="https://heqi867610481.github.io/KingNothing.github.io/tags/%E8%AE%BA%E6%96%87/" rel="tag">论文</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-paper7" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="https://heqi867610481.github.io/KingNothing.github.io/2020/06/10/paper7/" class="article-date">
  <time datetime="2020-06-10T02:53:31.000Z" itemprop="datePublished">2020-06-10</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="https://heqi867610481.github.io/KingNothing.github.io/2020/06/10/paper7/">[object Object]</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>BGM：伎俩 – 二手玫瑰</p>
<h1 id="1-论文总体概述"><a href="#1-论文总体概述" class="headerlink" title="1 论文总体概述"></a>1 论文总体概述</h1><p>论文做了一种对与一般的图像缩放算法的自动攻击，能够构造欺骗性图像，使图像在缩放算法处理前后视觉语意上发生很大的变化。因为对于很多现在流行的CNN图像检测模型（Caffe,TensorFlow和Torch），对输入图形进行缩放都是预处理过程中的重要步骤，所以这种攻击，在缩放后使图像视觉语义发生很大变化，能够对图片分类模型产生规避攻击或者数据毒化效果。实验方面，最后在基于云的图片服务上（Microsoft Azure,Aliyun,Baidu,Tencent）做了对效果的评估，能导致明显的误分类效果，即使图像处理（缩放算法和维度参数）的细节是不可知的。作为防护，论文也提出了一些潜在的对策。<br>攻击者实现攻击需要解决两个技术上的难题。一个是需要通过分析算法来决定在哪里插入像素点。另一个是，对于基于云的计算机视觉服务，确切的缩放算法和模型输入尺寸对于用户是不可知的，攻击者需要推断推断和缩放相关的参数。为了解决这些问题，首先缩放攻击的过程公式化为一个通用的优化问题。然后对于黑盒和白盒场景下的可行性都做了测试。对于白盒场景，针对的是Caffe,TensorFlow和Torch这三个通用的深度学习框架，结果发现几乎这些框架所使用的所有的默认图片缩放算法对于缩放攻击都很脆弱，攻击者可以对输入数据增加毒化或欺骗像素，而这些对于用户是可见的，但对于缩放算法则会被丢弃，从而被深度学习模型忽略。对于黑盒场景的攻击结果表明，即使处理过程的全部细节不可知，攻击仍然大部分有效。因为图片缩放模块是基于开源图像处理库或者开源的插值算法的，实现图像缩放的方式相对受限。攻击者可以暴力测试来推断缩放算法和目标规模。最后作者还发现并讨论了攻击延伸到一些计算机图像应用的影响范围。<br>论文主要贡献：1）揭示了图像缩放过程中的安全风险，对于常见的框架进行了验证。2）将缩放攻击公式化为一个受限优化问题，并展示了自动且有效的生成攻击图片的过程。3）证明了攻击对实施细节不可知的云服务依然有效。4）从预防和检测角度提出针对缩放攻击的防御策略。</p>
<h1 id="2-相关背景知识"><a href="#2-相关背景知识" class="headerlink" title="2 相关背景知识"></a>2 相关背景知识</h1><h2 id="2-1-图像缩放算法"><a href="#2-1-图像缩放算法" class="headerlink" title="2.1 图像缩放算法"></a>2.1 图像缩放算法</h2><p>图像缩放算法被设计来改变图像尺寸的同时仍保留视觉特征。图像缩放通过插值法来推测目标点的像素值。不同的缩放算法决定了用哪几个相邻像素点的值以及他们的权重分配。比如对于输出图片的每一个像素，最近邻算法仅从输入中选取一个最近的像素值来代替，而双线性算法考虑一系列在目标像素点的周围的像素值，然后计算这些值的加权平均作为分配给目标像素点的值。这些缩放算法通常假设图片中的像素值是自然的而不是恶意的像素级别的操作过的。而本文的攻击就是攻击者精心调整像素级别的信息从而改变图片的语意。</p>
<h2 id="2-2-深度学习的图片缩放预处理"><a href="#2-2-深度学习的图片缩放预处理" class="headerlink" title="2.2 深度学习的图片缩放预处理"></a>2.2 深度学习的图片缩放预处理</h2><p>对于图片分类模型，为了保证训练和分类的速度，图片通常都会被缩放到一个较小的尺寸（比如300*300）。即使一些模型的输入尺寸是固定的，比如特定大小的视频帧，经调查大多也是需要缩放这一过程的。<br><img src="https://heqi867610481.github.io/KingNothing.github.io/KingNothing.github.io/2020/06/10/paper7/images/9.jpg" alt="深度学习框架使用的缩放算法"><br>上图是论文中总结的常见的DL框架使用的缩放算法以及库。可以看出所使用的算法都是先水平缩放再垂直缩放。</p>
<h2 id="2-3-图片信息隐写（的区别）"><a href="#2-3-图片信息隐写（的区别）" class="headerlink" title="2.3 图片信息隐写（的区别）"></a>2.3 图片信息隐写（的区别）</h2><p>对于图片隐写而言，主要目的是通过图片插值来实现可逆信息的隐藏，但这与本文提出的攻击不同。1）目的不同，图片隐写的目的是在原图片中隐藏信息，从而使秘密信息对人类不可见，而缩放攻击的目的是在在原图片中隐藏目标图片，从而引起视觉歧义。2）图片隐写自定义的编码方法来隐藏或恢复信息（比如LSB和NIP），而这编码方式通常是保密的。但是缩放攻击相反，基于的是公开的插值算法。</p>
<h1 id="3-缩放攻击的公式化表示"><a href="#3-缩放攻击的公式化表示" class="headerlink" title="3 缩放攻击的公式化表示"></a>3 缩放攻击的公式化表示</h1><p><img src="https://heqi867610481.github.io/KingNothing.github.io/KingNothing.github.io/2020/06/10/paper7/images/10.jpg" alt="缩放攻击流程"><br>从上图中可以看到描述攻击过程的四个概念，srcImg（Sm<em>n）、attackImg（Am</em>n）、outImg（Dm<code>*n</code>）、targetImg（Tm<code>*n</code>）。但有些情况下，outImg和targetImg有可能是同一个。<br>对于攻击，可以分为两种：强攻击模式和弱攻击模式。强攻击模式指的是srcImg和targetImg都被指定，这种攻击更具挑战。弱攻击模式指的是仅有targetImg被指定，某些极端情况下，图片内容也可能是无意义的，仅仅是为了导致一个错误的分类结果。没有指定srcImg，攻击者的目的仅仅是希望使ScaleFunc函数前后的图片差别尽量大。<br>对于强攻击模式，srcImg和targetImg都被指定，还看上图。这个时候攻击者的任务就是生成一个能导致欺骗效果的attackImg。但是符合条件的attackImg并非只有一个，这是因为ScaleFunc函数是满射的，也就是说生成同样的输出结果outImg，可以对应有不同的attackImg。在众多符合条件的attackImg中选择最优，可以选择和S最相似的A，同时限制D和T的差异在一个上限之内。下图是强攻击模式下的公式表达，其中衡量两个图片的距离，用的是L范式。<br><img src="https://heqi867610481.github.io/KingNothing.github.io/KingNothing.github.io/2020/06/10/paper7/images/11.jpg" alt="强攻击目标函数公式"><br>对于弱攻击模式，没有指定的S，流程图中就没有S的存在了。这个时候目标是找到一个A，使得A和T差异最大，同时，同样的限制D和T的差异在一个可接受的上界之内。因此弱攻击下的公式表达如下图所示。<br><img src="https://heqi867610481.github.io/KingNothing.github.io/KingNothing.github.io/2020/06/10/paper7/images/12.jpg" alt="弱攻击目标函数公式"><br>在上图的公式的第一行，所用的ScaleFunc是把T的图片尺寸缩放到和A的大小。</p>
<h1 id="4-缩放攻击详细过程"><a href="#4-缩放攻击详细过程" class="headerlink" title="4 缩放攻击详细过程"></a>4 缩放攻击详细过程</h1><p>数学内容见论文第5章。</p>
<h1 id="5-实验过程"><a href="#5-实验过程" class="headerlink" title="5 实验过程"></a>5 实验过程</h1><p>对于三种类型进行了实验：本地的图片分类应用（白盒）、计算机视觉云服务（黑盒）、web浏览器。</p>
<h2 id="5-1-白盒"><a href="#5-1-白盒" class="headerlink" title="5.1 白盒"></a>5.1 白盒</h2><p>在这种情况下，假设攻击者知道模型要求的输入尺寸以及缩放算法。这可以通过逆向以及从已知信息推断等方法实现。实验针对Caffe,Tensorﬂow,和Torch三个流行的深度学习框架。对于每一个框架，本地实现了一遍BAIR/BVLC GoogleNet模型。此模型要求输入尺寸为224*224。实验的结果，大部分是成功的，但对于一部分不太常见的缩放算法，还没有生成有效的攻击，这是因为1）这些算法可能在缩放过程中设置了更多的限制，作者还没有学习到实施细节；2）在这篇论文中，仅仅对优化任务设置了一个严限制，在攻击效果和攻击图片生成难度上有个折衷。</p>
<h2 id="5-2-黑盒"><a href="#5-2-黑盒" class="headerlink" title="5.2 黑盒"></a>5.2 黑盒</h2><p>这种情况下对输入尺寸和缩放算法是未知的，目的是使对输入图片的误分类。对于黑盒的攻击分两步：第一步是缩放参数的推断，第二步是根据推断的参数来构造攻击图片。<br>这里详说下第一步，略第二步和实验结果。两条经验性的观察有助于实现对缩放参数的推断。首先可以观察到对于大多数的CNN模型，输入都要求是正方形的图片，边长是201~300。其次可以观察到最常用的默认算法是Nearest, Bilinear, 以及Bicubic。范围确定后，用穷举法来确定确切的参数。核心思想就是构造一系列针对不同的参数的探针图片，然后哪个分类正确，哪个参数就是对的。但是为了提高效率，可以用一种包含了多个子图片的复杂攻击图片。首先选择n个属于不同分类的子探针图片。第二用一张全白图片作为背景，并将其分成n个不相交的探针区域。第三重复以下步骤：1）用第j张子探针图片放在空白图片的第j块探针区域。2）用一套缩放参数来对他缩放。3）把空白图片当作srcImg，把缩放过的图片当作targetImg。最后攻击者把所有输出结合起来构造探针图片，这样当探针图片被缩放，只有参数对上了的第j个区域的图片会被还原。</p>
<h2 id="5-3-web浏览器"><a href="#5-3-web浏览器" class="headerlink" title="5.3 web浏览器"></a>5.3 web浏览器</h2><p>因为网页有时候也对图片内容提供缩放功能，攻击者可能利用这一点来进行欺骗攻击或者钓鱼。</p>
<h2 id="5-4-可能干扰缩放攻击的因素"><a href="#5-4-可能干扰缩放攻击的因素" class="headerlink" title="5.4 可能干扰缩放攻击的因素"></a>5.4 可能干扰缩放攻击的因素</h2><p>图片预处理过程除了缩放可能还包括其他步骤，比如剪裁、过滤等。如果这些步骤出现在缩放前，就有可能对缩放攻击造成干扰。这里详细讨论一下。<br>剪裁：对于输入的图片，为了数据增强或者移除背景的目的，截断特定的区域。剪裁操作通常改变图片的纵横比，而且如果缩放攻击针对的是错误的维度的话，图片也不能恢复到正确的targetImg。只有在一些特别的情况下才能经过剪裁仍然有效，那就是剪裁仍然保留了纵横比，并且应用的算法是Nearest。当然受影响的程度也和被剪裁掉的相对面积有关。<br>过滤：指的是模糊或者锐化，调整颜色调色板。这类操作改变了像素值，因此直接影响了缩放攻击。因为攻击是基于对插值法涉及到的相邻像素点的平均值的操作。对于简单的缩放算法，比如Nearest，经过这一步骤可能还能保留攻击效果。<br>精细变换：对输入图片的旋转或者镜像。对图片旋转任意角度很大程度上破坏了攻击者的精心构造。但是，根据输入尺寸和选择算法的不同，旋转180°，对图片镜像可能对缩放攻击无影响。一些缩放算法与方向无关，也就是缩放后的输出和输入时是从左向右输入像素值还是什么方向无关。在这种情况下，不会影响攻击。<br>尽管以上操作会对攻击有影响，但如果攻击者知道有什么操作，并且知道确切的参数，那就不构成问题了。这种情况下，这些操作都可以用矩阵表示，如果存在逆矩阵，攻击者就可以在把攻击图片输入之前，先应用一下逆矩阵。<br>除了以上操作外，还有一些图片自己的因素能影响攻击的效果，尤其是尺寸和明度。攻击者需要找到合适的source和target图片才能保证攻击效果。<br>尺寸：source和target图片的尺寸决定了有多少冗余像素能够被用来实施攻击。如果缩放前后的尺寸差异很小的话，就不足以产生成功的攻击效果。<br>明度：source和target图片的明度决定了限制的宽严。考虑最糟糕的情况，全白的source和全黑的target很难构造一个能够有欺骗效果的攻击图片。</p>
<h1 id="6-防御与检测手段"><a href="#6-防御与检测手段" class="headerlink" title="6 防御与检测手段"></a>6 防御与检测手段</h1><p>防御手段：最简单粗暴的方式就是删除和深度学习模型要求的尺寸不同的图片，但是这只有少数场合能用。（话说都这样了缩放步骤意义何在。）然后另一个解决办法是在缩放前成行成列的随机移除一些像素。<br>检测手段：因为缩放攻击是要在缩放前后造成视觉特征的巨大变化。一个可能的检测方案就是在缩放过程中检测特征的巨大变化，比如颜色直方分布以及颜色散射分布。<br>1）颜色直方分布。计算前后不同像素值的点的个数，形成两个256维的向量，然后计算他们的余弦距离。（计算时转换成灰度图）<br>2）颜色散射分布。衡量了颜色的分布特征。首先，对于拥有同样的像素值的所有像素点，计算他们到图片中心的平均距离，同样形成两个256维的向量，然后计算他们的余弦相似度。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://heqi867610481.github.io/KingNothing.github.io/2020/06/10/paper7/" data-id="ckbgdday40001focpcw015zt6" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="https://heqi867610481.github.io/KingNothing.github.io/tags/%E5%9B%BE%E5%83%8F%E7%BC%A9%E6%94%BE%E7%AE%97%E6%B3%95/" rel="tag">图像缩放算法</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="https://heqi867610481.github.io/KingNothing.github.io/tags/%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E7%94%9F%E6%88%90/" rel="tag">对抗样本生成</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="https://heqi867610481.github.io/KingNothing.github.io/tags/%E8%AE%BA%E6%96%87/" rel="tag">论文</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-paper6" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="https://heqi867610481.github.io/KingNothing.github.io/2020/05/30/paper6/" class="article-date">
  <time datetime="2020-05-30T09:25:15.000Z" itemprop="datePublished">2020-05-30</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="https://heqi867610481.github.io/KingNothing.github.io/2020/05/30/paper6/">论文笔记：6   Seeing isn’t Believing Towards More Robust Adversarial Attack Against Real World Object Detectors</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>BGM：Other Side – MIYAVI （我爱洞哥！）</p>
<h1 id="1-论文概述"><a href="#1-论文概述" class="headerlink" title="1 论文概述"></a>1 论文概述</h1><p>本文的对抗样本针对的是目标检测，而且做了在现实生活中的实验。论文将攻击提出两类：HA和AA。HA指的是隐藏攻击，指实际存在的物体逃避检测。AA指的是出现攻击，指让物体的图片海报被识别为一个实在的物体。针对HA，提出了特征干扰增强（feature-interference reinforcement，FIR）和增强现实限制生成（enhanced realistic constraints generation，ERG）两种方法来增强鲁棒性；针对AA，提出了nested-AE方法，结合了两种AE来对长距离和短距离进行攻击。此外，针对AA，还设计了不同的style，来使AA更不被发现。<br>实验结果表明生成的攻击可以在YOLO v3和faster-RCNN上超过SOTA的成果。同时也有很高的可移植性。可移植性部分实验了在其他黑盒模型上的表现，包括SSD（(Single Shot Detector），RFCN（Region based Fully Convolutional Network），和Mask RCNN。<br>之前的工作大多集中于数字空间的对图片分类器的攻击。最近，Tom et al.的研究将对抗样本拍照再输送图片<br>到分类器中，表明对抗攻击在物理世界的有效性。再最近，有一些在物理世界对抗目标检测的研究，他们提升AE的鲁棒性的主要方法是对AE进行变形，比如通过改变AE的大小来模仿不同的距离。但是这种办法能模拟的距离和角度很受限。所以现在需要的是AE在更长的距离和更广的角度起效。同时以往的研究在探究光线和背景对AE的影响方面也受限。所以本篇论文意图生成鲁棒的AE，可以面对不同距离、角度、光照等多种真实生活中的背景。<br>前文提到针对HA，提出了FIR和ERG两种方法。FIR具体指的是使生成的AE去影响隐藏层和最终层而不是仅仅影响最终的预测层。这样攻击者想隐藏的目标物体的特征就在分类过程中更早的被AE修改。ERG指的是用一系列合理的背景来自动生成对抗样本，即将合理的背景和形变后的目标物体合成在一起，这样生成的AE对现实世界的背景可以更鲁棒。<br>对于AA，提出的nested-AE，将不同距离的任务分解成两个任务：长距离的攻击和短距离的攻击，然后分别产生两个独立的AE，然后两个AE以一种嵌套的方式合成为1个AE。然后应用了前文提到的多种风格来使得AE效果更好，并且还应用了batch-variation来加速AE生成过程收敛。</p>
        
          <p class="article-more-link">
            <a href="https://heqi867610481.github.io/KingNothing.github.io/2020/05/30/paper6/#more">Read More</a>
          </p>
        
      
    </div>
    <footer class="article-footer">
      <a data-url="https://heqi867610481.github.io/KingNothing.github.io/2020/05/30/paper6/" data-id="ckatplm0i0000vccp2sjz2ek0" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="https://heqi867610481.github.io/KingNothing.github.io/tags/%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E7%94%9F%E6%88%90/" rel="tag">对抗样本生成</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="https://heqi867610481.github.io/KingNothing.github.io/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/" rel="tag">目标检测</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="https://heqi867610481.github.io/KingNothing.github.io/tags/%E8%AE%BA%E6%96%87/" rel="tag">论文</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-paper5" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="https://heqi867610481.github.io/KingNothing.github.io/2020/05/22/paper5/" class="article-date">
  <time datetime="2020-05-22T02:37:58.000Z" itemprop="datePublished">2020-05-22</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="https://heqi867610481.github.io/KingNothing.github.io/2020/05/22/paper5/">论文笔记：5   Procedural Noise Adversarial Examples for Black-Box Attacks on Deep Convolutional Networks</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>BGM：Crimson Bow and Arrow – Epica</p>
<h1 id="1-论文总体思路"><a href="#1-论文总体思路" class="headerlink" title="1 论文总体思路"></a>1 论文总体思路</h1><p>现存的输入不可知的对抗性扰动展示了一种有趣但现在无法解释的视觉模式。本篇论文引入了一种结构化的程序噪声函数来生成通用对抗扰动（UAP）的方法。这种方法在计算机图形化中广泛应用并且被设计为可参数化的。这种方式生成的噪声图样和现存的通用对抗扰动在视觉上有着相似性。此方法揭示了现在流行的DCN架构（比如Inception v3和YOLO v3）的系统脆弱性。此外，过程中用了贝叶斯优化来有效地学习程序噪音参数来构建低代价无目标的黑盒攻击。进一步的，启发了输入不可知的防御来提高模型对对抗扰动的稳定性。攻击方法的普遍性暗示了DCN可能对低层级类别不可知的特征的聚合很敏感。<br>实验过程，首先在大规模ImageNet classifier上进行黑盒攻击，误分类率高达98.3%。攻击也迁移到了目标检测任务中，结果表示对YOLO v3目标检测器的检测物体有模糊效果。<br>论文创新点：1）第一个对模型不可知黑盒生成通用对抗扰动的方法。2）利用贝叶斯优化来有效加强黑盒攻击。他提高了5折随机参数选择的查询效率，并且效果要好于现在流行的L-BFGS优化算法。3）实验证据表明，程序噪音UAP似乎利用DCN的低层级特征，这一脆弱性可能会被利用创造跨应用的通用对抗性扰动。</p>
        
          <p class="article-more-link">
            <a href="https://heqi867610481.github.io/KingNothing.github.io/2020/05/22/paper5/#more">Read More</a>
          </p>
        
      
    </div>
    <footer class="article-footer">
      <a data-url="https://heqi867610481.github.io/KingNothing.github.io/2020/05/22/paper5/" data-id="ckanu4eaq0000skcp11eldwxc" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="https://heqi867610481.github.io/KingNothing.github.io/tags/%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E7%94%9F%E6%88%90/" rel="tag">对抗样本生成</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="https://heqi867610481.github.io/KingNothing.github.io/tags/%E8%AE%BA%E6%96%87/" rel="tag">论文</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-paper4" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="https://heqi867610481.github.io/KingNothing.github.io/2020/05/17/paper4/" class="article-date">
  <time datetime="2020-05-17T11:47:27.000Z" itemprop="datePublished">2020-05-17</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="https://heqi867610481.github.io/KingNothing.github.io/2020/05/17/paper4/">论文笔记：4   Attacking Graph-based Classification via Manipulating the Graph Structure</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>BGM：King Nothing – Metallica （今天没打鼓，良心痛。看一篇不太了解的领域的论文，换下口味。）</p>
<h1 id="1-背景知识"><a href="#1-背景知识" class="headerlink" title="1 背景知识"></a>1 背景知识</h1><p>这篇论文是关于对基于图的分类的攻击的（好绕）。基于图的分类大致可分为这两种：Collective Classification和图神经网络（GNN）。然后这篇论文是针对Collective Classification这一类的。论文中说现在对抗攻击大多都是针对非图的算法，就算是针对图的，也是GNN比较多。对于某些特定的安全问题，Collective Classification效果要好一点 。在这里把论文提到的基于图分类的相关算法都列出学习一下。<br>先说一下数据的结构。给定的是一个图和训练集。图可以是有向图或者无向图。训练集是图上一部分标记为正节点和负节点的节点。基于图的分类就是要预测这些没标记的节点是正是负。</p>
        
          <p class="article-more-link">
            <a href="https://heqi867610481.github.io/KingNothing.github.io/2020/05/17/paper4/#more">Read More</a>
          </p>
        
      
    </div>
    <footer class="article-footer">
      <a data-url="https://heqi867610481.github.io/KingNothing.github.io/2020/05/17/paper4/" data-id="ckab75xm40000xocpargdd0lg" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="https://heqi867610481.github.io/KingNothing.github.io/tags/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" rel="tag">图神经网络</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="https://heqi867610481.github.io/KingNothing.github.io/tags/%E5%AF%B9%E6%8A%97%E6%94%BB%E5%87%BB/" rel="tag">对抗攻击</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="https://heqi867610481.github.io/KingNothing.github.io/tags/%E8%AE%BA%E6%96%87/" rel="tag">论文</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-paper3" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="https://heqi867610481.github.io/KingNothing.github.io/2020/05/16/paper3/" class="article-date">
  <time datetime="2020-05-16T02:22:52.000Z" itemprop="datePublished">2020-05-16</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="https://heqi867610481.github.io/KingNothing.github.io/2020/05/16/paper3/">论文笔记：3   Adversarial Attacks on Convolutional Neural Networks in Facial Recognition Domain</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>BGM：Heart’s Desire – Dreamtale</p>
<h1 id="1-论文概述"><a href="#1-论文概述" class="headerlink" title="1 论文概述"></a>1 论文概述</h1><p>首先应用FGSM对人脸图片添加扰动，并在在自己训练的不同分类器上测试了效果，以验证可移植性（白盒）。接下来又精心构造了一些攻击算法（偏黑盒），针对的是untargeted黑盒，目的是评估人脸识别领域的DNN的鲁棒性。具体有三种方式：给最优的一个像素改变很大值；给所有像素值改变一个很小值；或者二者结合。最后结果是第二种方式明显要好，而且即使是最高水平的扰动，扰动后的图片也是对于人类可识别的。论文关注的一个问题是：对于人脸识别是否有特别的特征可以让攻击更成功或者更不成功。</p>
        
          <p class="article-more-link">
            <a href="https://heqi867610481.github.io/KingNothing.github.io/2020/05/16/paper3/#more">Read More</a>
          </p>
        
      
    </div>
    <footer class="article-footer">
      <a data-url="https://heqi867610481.github.io/KingNothing.github.io/2020/05/16/paper3/" data-id="cka9n3cs80000d8cp8wom47k8" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="https://heqi867610481.github.io/KingNothing.github.io/tags/%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB/" rel="tag">人脸识别</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="https://heqi867610481.github.io/KingNothing.github.io/tags/%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E7%94%9F%E6%88%90/" rel="tag">对抗样本生成</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="https://heqi867610481.github.io/KingNothing.github.io/tags/%E8%AE%BA%E6%96%87/" rel="tag">论文</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-paper2" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="https://heqi867610481.github.io/KingNothing.github.io/2020/05/15/paper2/" class="article-date">
  <time datetime="2020-05-15T11:23:24.000Z" itemprop="datePublished">2020-05-15</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="https://heqi867610481.github.io/KingNothing.github.io/2020/05/15/paper2/">论文笔记：2   Fooling automated surveillance cameras：adversarial patches to attack person detection</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>BGM：Swan Lake – Dark Moor（这溢出耳机的黑魔法气息！）</p>
<h1 id="1-概述"><a href="#1-概述" class="headerlink" title="1 概述"></a>1 概述</h1><p>论文的源码<a href="https://gitlab.com/EAVISE/adversarial-yolo" target="_blank" rel="noopener">https://gitlab.com/EAVISE/adversarial-yolo
</a><br>首先说这个论文应该是把对抗样本应用到real-world的一个例子。他生成了一个patch（40cm x40cm）可以打印出来挂到身上来躲避person detection。一个难点或者创新点是人类具有intra-class variety，就是说同属于人类看起来差别很大，对比一下路边的stop路标。而且人的background也啥都有，而路标的背景就比较固定。论文针对的是YOLOv2物体检测。</p>
        
          <p class="article-more-link">
            <a href="https://heqi867610481.github.io/KingNothing.github.io/2020/05/15/paper2/#more">Read More</a>
          </p>
        
      
    </div>
    <footer class="article-footer">
      <a data-url="https://heqi867610481.github.io/KingNothing.github.io/2020/05/15/paper2/" data-id="cka8794px00007scp7la3g2n1" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="https://heqi867610481.github.io/KingNothing.github.io/tags/%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E7%94%9F%E6%88%90/" rel="tag">对抗样本生成</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="https://heqi867610481.github.io/KingNothing.github.io/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/" rel="tag">目标检测</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="https://heqi867610481.github.io/KingNothing.github.io/tags/%E8%AE%BA%E6%96%87/" rel="tag">论文</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-paper1" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="https://heqi867610481.github.io/KingNothing.github.io/2020/05/14/paper1/" class="article-date">
  <time datetime="2020-05-14T06:37:01.000Z" itemprop="datePublished">2020-05-14</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="https://heqi867610481.github.io/KingNothing.github.io/2020/05/14/paper1/">论文笔记：1   Face-Off：Adversarial Face Obfuscation</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>BGM：Dies Irae – Dark Moor（配合食用更佳）（没什么比output同时夹带私货更开心了）</p>
<h1 id="1-论文总体思路"><a href="#1-论文总体思路" class="headerlink" title="1 论文总体思路"></a>1 论文总体思路</h1><p>首先这篇设计的Face-Off针对的是人脸recognition而不是detection。目标是使服务提供者仍然能检测到人脸，但是却把他认错。<br>面临的挑战有 1）现存的对抗攻击多关注于分类器（直接输出分类结果），但是paper解决的问题是针对所谓“metric learning system”，这种人脸识别/验证都是把输入用特征向量表示，然后去找最近的一簇。2）paper面临的是要实现黑盒攻击，而且也不能利用黑盒的输出去生成对抗性样本。<br>对于困难1），解决办法是专门为“metric learning system”设计了两种损失函数。损失函数目的是在得到嵌入向量时就尽量让图片远离正确答案。<br>对于困难2），解决办法是利用移植的办法。先用一个替代模型（白盒）生成对抗样本，然后用一个因子阿尔法a放大扰动。用放大因子也有个好处就是减少了生成对抗样本的时间。<br>这个替代的白盒模型用了两种模型【25】【36】。生成对抗样本的攻击算法也用了两种，即CW和PGD。<br>然后Face-Off的工作流程是这样的：1）收到用户的目标图片；2）识别并分离出脸部；3）调整大小为之后的工作；4）为脸部生成扰动；5）返回原大小的扰动后的图像</p>
        
          <p class="article-more-link">
            <a href="https://heqi867610481.github.io/KingNothing.github.io/2020/05/14/paper1/#more">Read More</a>
          </p>
        
      
    </div>
    <footer class="article-footer">
      <a data-url="https://heqi867610481.github.io/KingNothing.github.io/2020/05/14/paper1/" data-id="cka6ei4t400004ocp48p63yc0" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="https://heqi867610481.github.io/KingNothing.github.io/tags/%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB/" rel="tag">人脸识别</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="https://heqi867610481.github.io/KingNothing.github.io/tags/%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E7%94%9F%E6%88%90/" rel="tag">对抗样本生成</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="https://heqi867610481.github.io/KingNothing.github.io/tags/%E8%AE%BA%E6%96%87/" rel="tag">论文</a></li></ul>

    </footer>
  </div>
  
</article>


  


  <nav id="page-nav">
    
    <span class="page-number current">1</span><a class="page-number" href="https://heqi867610481.github.io/KingNothing.github.io/page/2/">2</a><a class="extend next" rel="next" href="https://heqi867610481.github.io/KingNothing.github.io/page/2/">Next &amp;raquo;</a>
  </nav>

</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="https://heqi867610481.github.io/KingNothing.github.io/tags/%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB/" rel="tag">人脸识别</a></li><li class="tag-list-item"><a class="tag-list-link" href="https://heqi867610481.github.io/KingNothing.github.io/tags/%E5%90%8E%E9%97%A8%E6%94%BB%E5%87%BB/" rel="tag">后门攻击</a></li><li class="tag-list-item"><a class="tag-list-link" href="https://heqi867610481.github.io/KingNothing.github.io/tags/%E5%9B%BE%E5%83%8F%E7%BC%A9%E6%94%BE%E7%AE%97%E6%B3%95/" rel="tag">图像缩放算法</a></li><li class="tag-list-item"><a class="tag-list-link" href="https://heqi867610481.github.io/KingNothing.github.io/tags/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" rel="tag">图神经网络</a></li><li class="tag-list-item"><a class="tag-list-link" href="https://heqi867610481.github.io/KingNothing.github.io/tags/%E5%AF%B9%E6%8A%97%E6%94%BB%E5%87%BB/" rel="tag">对抗攻击</a></li><li class="tag-list-item"><a class="tag-list-link" href="https://heqi867610481.github.io/KingNothing.github.io/tags/%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E7%94%9F%E6%88%90/" rel="tag">对抗样本生成</a></li><li class="tag-list-item"><a class="tag-list-link" href="https://heqi867610481.github.io/KingNothing.github.io/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/" rel="tag">目标检测</a></li><li class="tag-list-item"><a class="tag-list-link" href="https://heqi867610481.github.io/KingNothing.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" rel="tag">神经网络</a></li><li class="tag-list-item"><a class="tag-list-link" href="https://heqi867610481.github.io/KingNothing.github.io/tags/%E8%AE%BA%E6%96%87/" rel="tag">论文</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="https://heqi867610481.github.io/KingNothing.github.io/tags/%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB/" style="font-size: 13.33px;">人脸识别</a> <a href="https://heqi867610481.github.io/KingNothing.github.io/tags/%E5%90%8E%E9%97%A8%E6%94%BB%E5%87%BB/" style="font-size: 10px;">后门攻击</a> <a href="https://heqi867610481.github.io/KingNothing.github.io/tags/%E5%9B%BE%E5%83%8F%E7%BC%A9%E6%94%BE%E7%AE%97%E6%B3%95/" style="font-size: 10px;">图像缩放算法</a> <a href="https://heqi867610481.github.io/KingNothing.github.io/tags/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" style="font-size: 10px;">图神经网络</a> <a href="https://heqi867610481.github.io/KingNothing.github.io/tags/%E5%AF%B9%E6%8A%97%E6%94%BB%E5%87%BB/" style="font-size: 13.33px;">对抗攻击</a> <a href="https://heqi867610481.github.io/KingNothing.github.io/tags/%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E7%94%9F%E6%88%90/" style="font-size: 16.67px;">对抗样本生成</a> <a href="https://heqi867610481.github.io/KingNothing.github.io/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/" style="font-size: 13.33px;">目标检测</a> <a href="https://heqi867610481.github.io/KingNothing.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" style="font-size: 10px;">神经网络</a> <a href="https://heqi867610481.github.io/KingNothing.github.io/tags/%E8%AE%BA%E6%96%87/" style="font-size: 20px;">论文</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="https://heqi867610481.github.io/KingNothing.github.io/archives/2020/06/">June 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="https://heqi867610481.github.io/KingNothing.github.io/archives/2020/05/">May 2020</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="https://heqi867610481.github.io/KingNothing.github.io/2020/06/28/paper10/">论文笔记：10   Latent Backdoor Attacks on Deep Neural Networks</a>
          </li>
        
          <li>
            <a href="https://heqi867610481.github.io/KingNothing.github.io/2020/06/25/paper9/">论文笔记：9   Neural Network Inversion in Adversarial Setting via Background Knowledge Alignment</a>
          </li>
        
          <li>
            <a href="https://heqi867610481.github.io/KingNothing.github.io/2020/06/15/paper8/">论文笔记：8   CloudLeak：Large-Scale Deep Learning Models Stealing Through Adversarial Examples</a>
          </li>
        
          <li>
            <a href="https://heqi867610481.github.io/KingNothing.github.io/2020/06/10/paper7/">[object Object]</a>
          </li>
        
          <li>
            <a href="https://heqi867610481.github.io/KingNothing.github.io/2020/05/30/paper6/">论文笔记：6   Seeing isn’t Believing Towards More Robust Adversarial Attack Against Real World Object Detectors</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2020 Meiyi Jiang<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="https://heqi867610481.github.io/KingNothing.github.io/" class="mobile-nav-link">Home</a>
  
    <a href="https://heqi867610481.github.io/KingNothing.github.io/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  
<link rel="stylesheet" href="https://heqi867610481.github.io/KingNothing.github.io/fancybox/jquery.fancybox.css">

  
<script src="https://heqi867610481.github.io/KingNothing.github.io/fancybox/jquery.fancybox.pack.js"></script>




<script src="https://heqi867610481.github.io/KingNothing.github.io/js/script.js"></script>




  </div>
</body>
</html>